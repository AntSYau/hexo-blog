---
title: 数据挖掘期末复习
date: 2019-06-10 19:37:10
mathjax: true
categories:
- ffuck
tags:
- data-mining
---

[课本](http://www.mmds.org): Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). *Mining of Massive Datasets* (2nd ed.). Cambridge, England: Cambridge University Press.

- - -

修订：重写了LSH算法的部分（修正了错误）。增加了DGIM算法。

修订：更正了Winnow算法中的错误。

修订：新增了BFR算法。

- - -

由于前6章在小测中已有提及，故只针对部分算法做简单总结。

# 第3章 相似项发现

## Jaccard 相似度

集合的 Jaccard 相似度为 $\dfrac{|S\cap T|}{|S\cup T|}$ 。Jaccard相似度可记为$\text{SIM}(S,T)$。

## 文档的 Shingling

一篇文章就是一个字符串。文档的*k*-shingle定义为其中任意长度为$k$的字串。于是，一篇文档可以表示成文档中出现一次或多次的*k*-shingle的集合。

可以不将子字符串直接用成shingle，而是将字符串哈希到桶，用桶编号作为shingle。

也可基于词取shingle。例如，每个无意义词（列表维护）后面3个词作为一个3-shingle。

## LSH算法(Locality-Sensitive Hashing)

局部敏感哈希的基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻；而原本不相邻的两个数据，在低维空间中也将有很大的概率不相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，避免在高维数据空间中寻找，因为在高维空间中会很耗时。有这样性质的哈希映射称为是局部敏感的。

LSH在此处的做法是，将所有的shingle运行局部敏感的哈希映射（假设映射到0或1），然后进行$n$次随机置换。统计每个置换的Signiture Matrix中第一个1的位置，从而得到一个长度为$n$的有序数对。对比两个目标的这一有序数对的汉明距离，其与$n$的比值即可估计相似度。即

$$P(\text{minHash}(A)=\text{minHash}(B))=\text{SIM}(A,B)$$

# 第4章 数据流挖掘

## Bloom 过滤器

一个 Bloom 过滤器由以下几部分组成：

1. $n$个位组成的桶，每个位的初始值都为0
2. 一系列哈希函数$h_1,h_2,\cdots,h_n$组成的集合，将键值映射到桶中的一个位上
3. 一系列键值组成的集合$S$

Bloom 过滤器的目的是让所有键值在$S$中的元素通过，而阻挡大部分键值不在$S$中的元素。对$S$中每个元素运行哈希函数，将被映射到过的所有桶均设为1。当键值为$K$的元素到达时，计算$h_1(k),h_2(k),\cdots,h_n(k)$。只有当对应的位全为1时，允许其通过。

## 独立元素数目统计

对于任意一个流元素$a$运行哈希函数$h$时，位串$h(a)$的的尾部将以一些0结束，也可能没有0。假设流中目前所有已有元素$a$的最大**尾长(tail length)**为$R$，则由**FM算法(Flajolet-Martin Algorithm)**，独立元素的数目大约为$2^R$个。

## 窗口内的计数问题

DGIM算法能够用$O(\log^2N)$位来表示大小为$N$的窗口。遵循以下规则：

- 桶最右的位置上总为1
- 每个1都在某个桶中
- 一个位置只能属于一个桶
- 桶的大小从最小一直变化到某个最大值，相同大小的桶不超过两个
- 所有桶的大小是2的幂
- 从右到左桶的大小不会减小

桶最右的位置为桶的时间戳。维护桶时，如果当前位为0，无需动作。

如果当前位为1，则

1. 创建一个size为1的bucket，设置其时间戳。
2. 如果已有三个size为1的bucket，则将其中最早的两个bucket合并为一个size为2的bucket，将这两个bucket中出现比较晚的的时间戳作为新bucket的时间戳。
3. 如果已有三个size为2的bucket，则将其中最早的两个bucket合并为一个size为4的bucket，如此更新下去。
4. 如果时间戳早于窗口，则将其删除。

# 第5章 链接分析的 PageRank 算法

Web可以想象成一个有向图，若网页$p_1$到$p_2$之间存在一条或者多条链接，则$p_1$到$p_2$有一条有向边。可以根据这样一张有向图定义一个Web**转移矩阵(transition matrix)**。

从一个节点开始，随机冲浪者访问图中节点的过程是一个**马尔可夫过程(Markov process)**。因此PageRank的基本公式为

$$\vec{v'}=\vec M\cdot\vec v$$

为防止随机冲浪者陷入循环，我们假设随机冲浪者按照Web有向图的出链前进的概率略小于1（也就是“抽税”，允许冲浪者小概率随机跳转到全图任意一个网页）。我们设这一恒定的略小于1的概率为一个常数$\beta$。因此迭代的公式变成了

$$\vec{v'}=\beta\vec M\cdot\vec v+(1-\beta)\vec e/n$$

其中$n$是网页的总数。经过有限次迭代，便可算出每个Page的Rank。

# 第6章 频繁项集

##购物篮模型与 A-Priori

**购物篮模型(market-basket model)** 是用于描述两类对象之间一种常见形式的多对多关系。每个购物篮(basket)是由多个项(item)组成的项集(itemset)。

直观上看，一个在多个购物篮中出现的项集为“频繁”项集。

- $s:=$ 支持度阈值(support threshold)
- 项集$I$的支持度(support) $:=$ 包含$I$为子集的购物篮数目
- $I$是频繁项集(frequent itemset) $:=I$ 的支持度不小于$s$。

频繁项集的抽取结果往往采用if-then的形式来表示，这些形如$I\rightarrow j$的规则被称为**关联规则(association rule)**，如果$I$中所有项出现在某个购物篮的话，那么$j$“有可能”页出现在这一购物篮。

关联规则$I\rightarrow j$的**可信度(confidence)**等于集合$I\cap\{j\}$的支持度与$I$的支持度的比值。因此我们可以使用A-Priori算法来检索频繁项对。

1. 建立两张表，第一张表将项的名称转化为$1$到$n$之间的整数。第二张表记录第$i$个项的出现次数。每次读取购物篮时，根据第一张表将第二张表相应角标的数组元素$+1$。
2. 第一遍扫描后，我们检查所有项的计数值，以确定哪些项构成单元素频繁项集。只给频繁项重新编号，编号范围时$1$到$m$。
3. 在第二遍扫描中，我们对两个频繁项组成的所有项对计数（由于项集是单调的，不可能遗落频繁项对）。
    - 对每个购物篮，在频繁项集表中检查哪些是频繁的，
    - 然后通过循环生成所有频繁项对，对于每个频繁项对，在存储计数值的数据结构中在相应的计数值上$+1$。
    - 在第二遍扫描结束时，检查计数值结构以确定哪些项对为频繁项对。

项集具有**单调性(monotonicity)**，如果项集$I$是频繁的，那么其所有子集都是频繁的。

## PCY算法

PCY算法是A-Priori的改进。

在进行第一次扫描时，另外将所有1-项集组成的2-项集哈希到桶中。

进行第二次扫描时，每个候选项集不但要满足是频繁项集的子集，还要满足这一2-项集在第一次扫描时被哈希到了一个频繁桶中（桶中元素的计数大于一给定阈值）。

## SON算法

SON算法应用两次Map-Reduce。

第一个`Map`函数发现分配到的购物篮子集中的频繁项集。如果子集长度占输入文件比例为$p$，则这一步的支持度阈值从$s$降为$ps$。输出为一个$(F,1)$组合，其中$F$是一个频繁项集。

第一个`Reduce`函数函数出现一次或多次的键。即，生成`候选项集`。

第二个`Map`函数接收`候选项集`的所有输出和输入文件的一部分。每个`Map`计算候选项集在当前的购物篮数据集上的出现次数。输出$(C,v)$，其中$C$是一个候选集（不是频繁项集！），$v$是$C$在这一部分上的支持度。

第二个`Reduce`函数将$C$看成键，对$v$求和。最后，输出全部支持度高于$s$的项集。

#第7章 聚类

##两种聚类方法

**层次(hierarchical)** 或 **凝聚式(agglomerative)** 算法。一开始将每个点都看成一个 **簇(cluster)** 。簇与簇按照 **接近度(closeness)** 来组合。当进一步的组合会导致非期望结果时，组合结束（例如，达到给定的簇的数目，或者簇内点分散的区域过大）。

**点分配(point assignment)** 过程。按照某个顺序依次考虑每个点，并将它分到最适合的簇中。

## 层次聚类

####欧氏空间下的层次聚类

````
WHILE it is not time to stop DO
    pick the best two clusters to merge;
    combine those two clusters into one cluster;
END;
````

### 效率更高的算法实现方式

1. 第一步，计算所有点对的距离，将其放到一个优先队列中
2. 合并$C$和$D$时，删除队列中包含其中一个簇的所有元素。由于最多删除$2n$次，删除的时间开销为$O(n\log n)$。
3. 计算新簇和所有剩余簇的所有距离。由于最多插入$n$次，计算过程的时间开销同样是$O(n\log n)$。

由于最后两步最多执行n次，故总时间开销为$O(n^2\log n)$。

### 非欧空间下的层次聚类

处于非欧空间时，我们必须要使用 Jaccard 距离等进行距离计算。

有几种常用的选择中心点的算法，它们分别使得选出该点到簇中其他所有点之距离的如下数值最小：

1. 求和
1. 最大值
1. 平方和

## $k$-means 算法

````
Initially choose k points that are likely to be in different clusters;
Make these points the centroids of their clusters;
FOR each remaining point p DO
    Find the centroid to which p is closest;
    Add p to the cluster of that centroid;
    Adjust the centroid of that cluster to account for p;
END;
````

###簇初始化

簇初始化有两种做法。

1. 选择彼此距离尽可能远的点
1. 对某个样本数据先进行聚类，因此输出$k$个簇。在每个簇中选择一个点，例如离簇质心最近的那个点。

对于第1种做法，最常见的实现方案是：

```
Pick the first point at random;
WHILE there are fewer than k points DO
    Add the point whose minimum distance
        from the selected points
        is as large as possible;
END;
```

## BFR算法

假设簇满足以质心为期望的正态分布。按上述方式选择初始的k个点。

之后，数据文件中的点按照组块方式读入。

目前内存中有：

1. 组块
2. 废弃集
3. 压缩集
4. 留存集

下面开始数据处理。

1. 首先，充分接近某个簇质心的点会被加到该簇中。
1. 对于不充分接近簇质心的点，同留存集中的点一起聚类。
1. 压缩集迷你簇和新的迷你簇（迷你簇含有互相接近，但是不接近于任何簇的点）之间可以进行合并。
1. 不在留存集中的点，也就是一个簇或者迷你簇中的点，会和分配结果一起写到二级存储上。

$d$维点的压缩集和废弃集中拥有$2d+1$个值，包含$N-SUM-SUMSQ$：

1. $N:=$ 所表示的点数$N$
2. $SUM:=$ 所有点在每一维度的分量之和
3. $SUMSQ:=$ 所有点在每一维度的分量平方和

## CURE算法

该算法假定在欧氏空间下，对簇的形状没有任何假设，不需要满足正态分布，可以拥有S型甚至环形。

### CURE算法的初始化

1. 抽取一部分数据进行聚类（建议使用层次聚类）
2. 从每一个簇选择一小部分点集作为**簇的代表点**。选择时使用簇初始化的第1种做法。
3. 对每个代表点移动一段距离，该距离是其位置到簇质心距离乘以一个固定比例，例如，20%。

### CURE算法的完成

下一步是，当两个簇的某对代表点之间足够接近，那么就将这两个簇进行合并。用户可以定义“接近的簇”。该合并过程可以重复。

# 第8章 Web广告

##在线算法与贪心算法

Web广告相关算法是在线(on-line)算法，在算法必须做出某些决定之前并不能获得全部的数据。

很多在线算法都属于贪心算法(greedy algorithm)。Web广告的一个明显的贪心算法为，将查询分配给还有预算的出价更高的广告商。

对于同一问题，在线算法不如最佳的离线算法效率那么好。如果存在一个小于1的常数$c$表示某一个具体的在线算法的结果至少是最优离线算法结果的$c$倍，就说其为在线算法的**竞争率(competitive ratio)**。

## 广告匹配问题

假定给定一个二部图。一个 **匹配(matching)** 指二部图的子边集，使得原二部图只保留这一子集包含的边时，每一个节点的度不超过1。

如果某一个匹配中所有的节点都有边连接，那么这一匹配是 **完美(perfect)** 的。图中所有匹配中最大的那个匹配称为最大(maximal)匹配。

### 最大匹配贪心算法

最大匹配贪心算法的工作流程如下。我们可以按照任意次序来考虑边。当考虑边$(x,y)$时，如果$x$和$y$都不是已有匹配中边的端点，则将其加入，否则跳过。这一算法的竞争率为$1/2$。

## Adwords问题

- 给定下列信息：
    1. 广告商的投标价格
    2. 每个广告商-查询的点击率
    3. 每个广告商的预算
    4. 每个搜索查询所显示的广告数目上限
- 对每个搜索查询，算法给出一系列广告商的应答结果的集合，满足：
    1. 集合大小不超过广告数目上限
    2. 每个广告商都对本条查询出了价
    3. 每个广告商必须有足够预算来付费

依次广告选择的收益(revenue)是每个选出广告的价值之和，其中每条广告的价值(value)等于对应查询的出价和广告点击率的乘积。

### Balance算法

Balance算法是Adwords问题的贪心算法的改进版本。当广告商出价只能是0和1时，它将查询分配给出价最高且剩余预算最多的广告商。如果多个广告商的剩余预算相等，那么可以随意地选择任意一个。

当只有两个广告商时，Balance算法的竞争率为$3/4$。然而随着广告商数目的不断增长，竞争率会趋于$1-1/e$。

### *Balance 算法的一般化

为了使Balance算法适用于更一般的情况，需要做如下修改：

- 选择时，必须要倾向于出价高的广告
- 对剩余预算的处理不能那么绝对。这是为了规避风险，使任意广告商的剩余预算不会太多。

假设某个查询$q$到达，广告商$A_i$对$q$的出价为$x_i$（可以为0）。另外假定$A_i$预算中的结余比率占$f_i$。令$\Psi_i=x_i(1-e^{-f_i})$。那么将$q$分配给具有最大$\Psi_i$值的广告商$A_i$。

#第12章 大规模机器学习

##机器学习模型

应用机器学习算法的数据称为训练集。训练集由一系列$(x,y)$对组成，每个$(x,y)$称为一个**训练样例(training example)**，其中：

- $x$是一个**特征向量(feature vector)**
- $y$是一个**标签(label)**

ML过程的目标是寻找一个函数$y=f(x)$来预测对于每个$x$的最佳$y$值。$y$的常见重要类型：

- $y$是实数
- $y$是布尔值，或$+1$和$-1$ **$\Rightarrow$ 二类分类(binary classification)** 
- $y$是有穷集合中的元素 **$\Rightarrow$ 多类分类(multiclass classification)** 

### 主要的机器学习算法类别：

- 决策树(decision tree)，类似于搜索树
- 感知机(preception)，对向量中各分量加权求和，若和大于给定阈值$\theta$则输出$+1$。适用于二类分类问题
- 神经网络(neural net)，感知机组成的网络。
- 基于实例的学习(instance-based learning)，使用整个训练集作为$f$。

## 感知机

为训练感知机，我们来考察一个训练集并试图寻找一个权重向量$\vec{w}$和阈值$\theta$。一开始假定阈值为0，在此基础上针对未知阈值应用以下算法，使其收敛到一个将训练集中所有正例和负例分开的超平面：

1.  将权重向量$\vec{w}$的所有元素初始化为0；
2.  选择一个很小的正实数$\eta$作为**学习率参数(learning parameter)**。$\eta$的选择会影响感知机的收敛。
3.  依次考虑每个训练样例$t=(\vec{x},y)$:
    - 令$y'=\vec{w}\cdot\vec{x}$;
    - 如果$y$和$y'$的符号相同，则什么都不做，$t$已正确分类；
    - 如果$y$和$y'$的符号不同或$y'=0$，那么将$\vec{w}$替换成$\vec w+\eta y\vec x$。也就是说，沿$\vec x$方向微调$\vec w$。

#### *算法终止的常见方法：

1. 固定次数时终止
2. 分错类的数据点的数目不再变化时终止
3. 抽样，在样本上执行2
4. 降低训练率，例如在第$t$次迭代中将训练率$\eta$变为$\eta_0/(1+ct)$，其中c是很小的常数。

### Winnow 算法

Winnow假定所有的特征向量都由0和1组成，类别用$+1$和$-1$表示。Winnow算法的权重向量都是正值。

Winnow算法的基本思想是，只降低或增加分错类的数据点中与正确类之符号相同或相反的分量的权重。

例如，以2和1/2作为因子分别代表提高权重和降低权重的情况：

1. 如果$\vec w \cdot \vec x > \theta$且$y=+1$，或者$\vec w \cdot \vec x < \theta$且$y=-1$，那么该样本被正确分类，此时对$\vec w$不做任何更改。
2. 如果$\vec w \cdot \vec x \leq \theta$但$y=+1$，那么$\vec x$中为1的分量所对应的$\vec w$的权重太低。此时，对这些权重进行加倍操作。也就是说，如果$x_i=1$，那么令$w_i:=2w_i$。
3. 如果$\vec w \cdot \vec x \geq \theta$但$y=-1$，那么$\vec x$中为1的分量所对应的$\vec w$的权重太高。此时，对这些权重进行减半操作。也就是说，如果$x_i=1$，那么令$w_i:=w_i/2$。

####允许阈值变化的情况：

- 将权重向量$\vec w$定义为 $$\vec w'=[w_1,w_2,\cdots,w_d,\theta]$$
- 将特征向量$\vec x$ 定义为 $$\vec x'=[x_1,x_2,\cdots,x_d,-1]$$

对于新的训练样本和权重向量，可以认为阈值为0并运行基本的感知机算法。

或者可以认为是 Winnow 算法。只不过在其它权重需要翻倍的时候，阈值对应的权重除以2；其他权重需要除以2的时候，阈值对应的权重要翻倍。

## *支持向量机

**支持向量机(Support-Vector Machine, SVM)** 的目标是选择一个超平面$\vec w\cdot\vec x+b=0$，使得该超平面到训练集中任意点距离$\gamma$的最小值最大。

换句话说，就是给定训练集$(\vec{x_1},y_1),(\vec{x_2},y_2),\cdots,(\vec{x_n},y_n)$，对于所有$i=1,2,\cdots,n$，在若狭约束条件下通过变化$\vec w$和$b$最小化$||\vec w||$: $$y_i(\vec w \cdot \vec x+b)\geq1$$

由于$\gamma=1/||\vec w||$，最小化$||\vec w||$就是最大化$\gamma$。